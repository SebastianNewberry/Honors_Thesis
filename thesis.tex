\documentclass[onecolumn,10pt]{article}

% -----------------------
% Fonts & encoding
% -----------------------
\usepackage[utf8]{inputenc} % ok for pdfLaTeX
\usepackage[T1]{fontenc}
\usepackage{lmodern}        % fallback scalable fonts
% If you prefer a Times-like document, use newtx (uncomment next two lines)
\usepackage{dirtree}  % for directory tree visualization
\usepackage{newtxtext,newtxmath}

% -----------------------
% Layout & microtype\paragraph{\LARGE\bfseries SmileyCTF 2025 -- SaaS (Cryptography)\quad\textit{(Difficulty: Easy)}}
% -----------------------
\usepackage[margin=1in,top=0.9in,bottom=1in,left=0.9in,right=0.9in]{geometry} % single canonical geometry call
\usepackage{microtype}
\sloppy                              % tolerate a bit more raggedness to avoid overfull hboxes
\setlength\emergencystretch{2em}     % help LaTeX avoid overfull lines

% -----------------------
% Math, symbols, and theorem tools
% -----------------------
\usepackage{amsmath,amssymb,amsthm}
\usepackage{mathtools}  % loads amsmath extras
\usepackage{tikz}  % for drawing diagrams
% -----------------------
% Graphics, floats & captions
% -----------------------
\usepackage{graphicx}
\usepackage{float}
\usepackage{wrapfig}
\usepackage{booktabs}
\usepackage{caption}
\usepackage{subcaption}
\captionsetup[subfigure]{subrefformat=parens}


% -----------------------
% Lists, spacing, titles
% -----------------------
\usepackage{enumitem}
\setlist{itemsep=0.3em, topsep=0.5em}
\usepackage{titlesec}
\titleformat{\section}{\normalfont\Large\bfseries}{\thesection}{1em}{}
\titleformat{\subsection}{\normalfont\large\bfseries}{\thesubsection}{1em}{}
\titleformat{\subsubsection}{\normalfont\normalsize\bfseries}{\thesubsubsection}{1em}{}

% -----------------------
% Code listings
% -----------------------
\usepackage{listings}
\lstset{
    literate=
      {—}{{---}}1
      {–}{{--}}1
      {…}{{...}}1
      {“}{{``}}1
      {”}{{''}}1
}
\usepackage{xcolor}
\lstdefinestyle{mypython}{
  language=Python,
  basicstyle=\ttfamily\footnotesize,
  keywordstyle=\color{blue},
  commentstyle=\color{gray},
  stringstyle=\color{orange},
  showstringspaces=false,
  breaklines=true,
  breakatwhitespace=true,
  tabsize=2,
  numbers=left,
  numberstyle=\tiny,
  frame=single
}

% -----------------------
% Spacing, title & header
% -----------------------
\usepackage{setspace}
\usepackage{parskip}  % better paragraph spacing than manual \vskip
\usepackage{fancyhdr}
\usepackage{titling}
\usepackage{abstract}

\lstdefinestyle{mypython}{
  language=Python,
  basicstyle=\ttfamily\footnotesize,
  keywordstyle=\color{blue},
  commentstyle=\color{gray},
  stringstyle=\color{orange},
  showstringspaces=false,
  breaklines=true,
  breakatwhitespace=true,
  tabsize=2,
  numbers=left,
  numberstyle=\tiny,
  frame=single
}

% -----------------------
% Spacing, title & header
% -----------------------
\usepackage{setspace}
\usepackage{parskip}  % better paragraph spacing than manual \vskip
\usepackage{fancyhdr}
\usepackage{titling}
\usepackage{abstract}
\usepackage{csquotes}


% -----------------------
% Hyperlinks (load near end)
% -----------------------
\usepackage{hyperref}
\hypersetup{
  colorlinks=true,
  linkcolor=blue,
  citecolor=blue,
  urlcolor=blue,
  pdfauthor={Sebastian Newberry},
  pdftitle={Solving CTF Cryptography Problems with LLMs}
}

\usepackage[
  backend=biber,
  style=apa,
  url=true,
  doi=true,
  isbn=false
]{biblatex}

\usepackage[capitalise,nameinlink]{cleveref}
\crefname{figure}{Figure}{Figures}
\Crefname{figure}{Figure}{Figures}
\crefname{subfigure}{Figure}{Figures}
\Crefname{subfigure}{Figure}{Figures}
\crefname{table}{Table}{Tables}
\Crefname{table}{Table}{Tables}

\addbibresource{references.bib}

% ---------- Geometry ----------
\geometry{
  top=0.9in,
  bottom=1in,
  left=0.9in,
  right=0.9in
}

% ---------- Title Customization ----------
\pretitle{\begin{center}\Large\bfseries\rule{\linewidth}{0.8pt}\\[0.5em]}
\posttitle{\\[0.5em]\rule{\linewidth}{0.8pt}\end{center}}
\preauthor{\begin{center}\normalsize}
\postauthor{\end{center}}
\predate{\begin{center}\small}
\postdate{\end{center}}

\title{Can Open-Source Large Language Models Replace Proprietary Closed-Source Models? An Empirical Study of CTF Problem-Solving Accuracy}
\author{Sebastian Newberry}
\date{\today}



% ---------- Section Formatting ----------
\titleformat{\section}{\normalfont\Large\bfseries}{\thesection}{1em}{}
\titleformat{\subsection}{\normalfont\large\bfseries}{\thesubsection}{1em}{}
\titleformat{\subsubsection}{\normalfont\normalsize\bfseries}{\thesubsubsection}{1em}{}

\makeatletter
\renewcommand{\maketitle}{%
  \begin{center}
    {\LARGE\bfseries\@title\par}\vskip0.5em
    {\normalsize\@author\par}\vskip0.5em
    {\small\@date\par}
    \rule{\linewidth}{0.8pt}\par
  \end{center}
}
\makeatother

\begin{document}

\setlength{\abovedisplayskip}{6pt}
\setlength{\belowdisplayskip}{6pt}

\maketitle


\begin{abstract}
  Open-source large language models (LLMs) have closed much, but not all, of the gap with frontier proprietary systems, and their relative standing varies sharply by benchmark family. On knowledge-heavy and long-horizon reasoning tasks (e.g., Humanity’s Last Exam, GPQA-Diamond, MMLU-Pro), top closed models such as GPT-5, Claude Sonnet 4.5, and Grok 4 generally retain an edge. By contrast, recent open models (DeepSeek, GLM-4.6, Kimi K2) often can match peers in coding and web-agent tasks (e.g. SWE-bench Verified), especially when tool use is allowed. Still, performance is benchmark-sensitive: DeepSeek R1/V3 trails on human-preference arenas despite strong math/coding abilities, GLM-4.6 shines on code but not always on long-form reasoning, and Kimi K2 leads some agentic evaluations yet lags top closed-source models on others. In this report, both closed and open source LLMs will be tested to determine their ability to apply knowledge to complex cybersecurity issues. In order to do this, CTF challenges will be used in an attempt to emulate a real cyber environment where a company could decide to use an LLM. A CTF (capture the flag) challenge is a problem where users are tasked to exploit vulnerabilities in a practice cybersecurity environment. Once the user is successful in exploiting the practice environment, they receive a string of text which is the flag. Many times these challenges can have multiple solutions, but they almost all require complex thinking and problem-solving abilities to work through them. These CTF challenges are all multi-step problems that require lots of thinking and effort to solve correctly. Measuring the ability for LLMs to solve cybersecurity challenges presents a unique trial for them to apply mathematic and programming knowledge to the real world.
  
  As for the specific challenges being used, I took some challenges from the Wayne State University CTF in 2025. This was a CTF ran in 2025 where myself and others got together to create a CTF competition where people around the world could form teams and play together. All of the challenges from the Wayne State University CTF were challenges I made. In this paper, I will also compare the general performance of these LLMs to the performance of actual people trying to solve my challenges, in order to find out whether LLMs can be used to automate cybersecurity tasks effectively. Additional challenges will also be showcased from other CTFs to show the difference between my own CTF and another CTF when it comes to the ability for an LLM to solve these types of problems.
\end{abstract}

\clearpage

\tableofcontents

\clearpage

\section{Introduction}
Companies today are turning to automated solutions like large language models to penetration test their infrastructure. Closed-source models like OpenAI's GPT, Anthropic's Claude, and xAI's Grok offer strong performance, but also offer drawbacks in terms of different aspects of the CIA triad that are vital to cybersecurity. The CIA triad stands for confidentiality, integrity, and availability. Each aspect of this triad is challenged in some way when a company decides to rely on a commercial LLM over an open-source LLM that is hosted on local infrastructure.

\subsection{CIA Triad}

\subsubsection{What is the CIA Triad}

The CIA triad stands for Confidentiality, Integrity, and Availability. According to Geeks4Geeks, the CIA Triad is a foundational model in information security \parencite{geeksforgeeks_cia_triad}.

\begin{itemize}
  \item \textbf{Confidentiality}: Ensures that sensitive data is accessible only to authorized users and protected from unauthorized disclosure or access.
  \item \textbf{Integrity}: Maintains the accuracy and reliability of data, ensuring it has not been altered or tampered with by unauthorized individuals.
  \item \textbf{Availability}: Guarantees that data, systems, and resources remain accessible to authorized users when needed, minimizing downtime and disruptions.
\end{itemize}

Overall, this serves as a guide to companies on to how to properly protect, maintain, and upkeep internal systems, networks, and customer data policies. 

\subsubsection{Confidentiality}  
When a provider fine-tunes or prompts a model on customer data, that content may be stored or reused for future training. Once proprietary threat data leaves a company's internal network, it becomes subject to the vendor’s retention, access, and legal processes. This poses unique risks for both blue and red teams. Defenders may lose control of sensitive detection logic, and red team operators could expose internal testing tools or exploit chains to the public. Running models locally removes this risk because prompts stay within the company, and all data remains under the company's own control. This exposes the confidentiality principle of cybersecurity because confidentiality involves being secretive about both business practices, and customer data. When you are using a closed-source model like Claude's Anthropic, you are giving them full permission to do what they want with your provided input.

According to the \textit{Anthropic terms of service}, \begin{quote}We may use Materials to provide, maintain, and improve the Services and to develop other products and services, including training our models, unless you opt out of training through your account settings. Even if you opt out, we will use Materials for model training when: (1) you provide Feedback to us regarding any Materials, or (2) your Materials are flagged for safety review to improve our ability to detect harmful content, enforce our policies, or advance our safety research.\end{quote} \parencite{anthropic_consumer_terms}


This snippet from the Anthropic terms of service shows that this company retains the right to use your data to train its proprietary models. Other closed source providers like OpenAI and xAI have very similar policies. They phrase their terms of service to make it sound like companies can easily opt out of any sort of data training, but behind the scenes, there is no way to truly protect this data without switching to an open source solution.
\subsubsection{Integrity}
Large language model providers face intense pressure to moderate and censor model outputs when user interactions trigger sensitive issues. For instance, in November 2025, a lawsuit alleged that ChatGPT encouraged a user to commit suicide rather than redirect him to proper care, spurring public backlash and regulatory scrutiny \parencite{chatgpt_suicide_lawsuit}.

Because of the risk of such outcomes, model responses are restricted, flagged for safety, or routed through safer versions of the model. These measures are intended to protect users, but they are simultaneously reducing the model’s openness and spontaneity, limiting how far users can push prompts or explore unusual content. In practical terms, this means someone trying to test the model’s full creative or adversarial potential may find their session abruptly truncated or redirected to bad responses. In the context of red-teaming or white-hat testing of models, what begins as free exploration can quickly convert into “safe mode” or refusal behavior. Often times when end users ask these AI models things like, "Can you help me hack into this system?" The AI will refuse because it is unethical. For blue teams responsible for defensive cybersecurity operations, this means that model access may be constrained when they ask the model to simulate threat actor behaviour or craft exploit chains. The system may refuse or degrade answers, citing policy violation. The red team that is trying to push the model to its limits when it comes to hacking test environments will also encounter this same issue. The result is a platform that must walk the line between usability and stringent censorship which isn't ideal.

This censorship concern has been shown in the past with DeepSeek censoring political topics that speak negatively about the Chinese government or CCP. Although this is true, since DeepSeek is open-source, it can be fine-tuned and adjusted by hobbyists and large providers to remove some of this bias and censorship from the model.

According to Wired, "Hugging Face is also working on a project called Open R1 based on DeepSeek’s model. This project aims to 'deliver a fully open-source framework,' Yakefu says. The fact that R1 has been released as an open-source model 'enables it to transcend its origins and be customized to meet diverse needs and values.'" \parencite{wired_deepseek_censorship}

In this conference paper by Noels, the authors of this paper thoroughly studied the censorship of AI by classifying it into soft and hard censorship \parencite{noels_llm_censorship}:

The authors of this conference paper conducted an experiment in order to determine how drastic censorship is in LLMs. The experiment takes a dataset of 300,000 political figures and the authors take steps to get the best results from LLMs on whether or not they choose to answer questions about the political figure. This experiment modeled testing different countries of origin for politicians against the amount of hard and soft censorship instances for different AI models. It also measured censorship results for different large language models based on the language that was used to prompt the model.

According to the conference paper:

\begin{quote}
  The prompting strategy is simple: each LLM in each language is asked about each political figure "Tell me about [Person X]." Based on the subselections listed above, we retain 156,486 responses to such prompts in total, of which 8.8\% are marked as hallucinations (see Appendix A) and 3.3\% as refusals. Because of the open-ended nature of the prompts, refusals rates tend to be far lower than in experiments where LLMs are directly subjected to political questionnaire tests.
\end{quote}

\begin{figure}[H]
  \centering
  \includegraphics[width=0.5\textwidth]{hard_vs_soft_censorship.png}
  \caption{Different types of hard and soft censorship}
  \label{fig:types-of-censorship}
\end{figure}

Figure~\ref{fig:types-of-censorship} defines soft censorship into "omission of praise" which refers to censorship that refuses to give credit to someone for positive things, and "omission of allegation" which refers to refusing to critize someone for negative things. It defines hard censorship as refusing to answer questions at all by either giving another internet source to answer for it (canned refusal), generating a response telling the user that it can't respond (generated refusal), or refusing to respond at all by throwing an API error or returning no text (error refusal).

\begin{figure}[H]
    \centering

    \begin{subfigure}[t]{0.48\textwidth}
        \centering
        \includegraphics[width=\linewidth]{hard_censorship_stats.png}
        \caption{hard censorship statistics}
        \label{fig:hard-censorship}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.48\textwidth}
        \centering
        \includegraphics[width=\linewidth]{selective_omission.png}
        \caption{soft censorship statistics}
        \label{fig:soft-censorship}
    \end{subfigure}

    \caption{Censorship and political leaders in LLMs based on prompting language and person's country of birth}
    \label{fig:hard-and-soft-censorship}
\end{figure}

\cref{fig:hard-and-soft-censorship} (a) and (b) demonstrate the results of the experiment that these authors conducted. In these results, it is shown that LLMs don't often exhibit hard censorship on political leaders besides the GigaChat model for the Russian language shows hard censorship in \cref{fig:hard-and-soft-censorship} (a). This is because most likely, Russian leaders don't want the LLM talking bad about them, so they most likely censored the model as a result. In \cref{fig:hard-and-soft-censorship} (b), it is shown that Wenxiaoyan and YandexGPT have high rates of both praise and accusation omission. The results out of this study also show that soft censorship occurs much more often than hard censorship which is a bad thing for users of these artificial intelligence platforms because soft censorship involves the AI censoring by excluding relevant information instead of refusing to answer. This would be terrible for something like an automated red team exercise where an AI excludes information about how to hack into a system in its response because of soft censorship. Users of Artificial Intelligence for cybersecurity purposes would much rather have an AI refuse to answer a question than provide wrong, or incomplete information due to censorship.
\subsubsection{Availability}  
Penetration tests are often run during maintenance windows or incident-response escalations that tolerate zero external dependencies.  Commercial LLM APIs, however, can be rate-limited, throttled, and occasionally taken offline for hours or days during regional outages or capacity rebalancing.  A red-team exercise that stalls because of an availability failure that can have negative effects on a company's bottom line.  Hosting open source models on internal GPU clusters can ensure that spontaneous third-party outages don't affect a company's infrastructure.

\section{Literature Review}

\subsection{General Benchmarks}

To determine whether a new model represents an improvement, AI researchers rely on benchmarks. Benchmarks are standardized, validated question sets that evaluate specific abilities or properties of a large language model. By using these shared tests, researchers can compare the performance of a new model against earlier versions or against competing models in a fair and reproducible manner.

\subsubsection{MMLU, GPQA, HLE, and SWE Bench-Verified}

\textbf{Massive Multitask Language Understanding (MMLU)} is a benchmark focused on measuring the ability for language models to complete multi-task problems in a variety of domains. MMLU-PRO was designed to improve on this benchmark. The questions in the original MMLU benchmark mostly consist of multiple choice questions that mostly only required knowledge to solve and not reasoning. Figure 3 shows some examples of what a question coming from the MMLU benchmark looks like:

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\textwidth]{MMLU_Sample_Questions.png}
    \caption{Examples from the Conceptual Physics and College Mathematics STEM tasks. \parencite{hendrycks2021measuringmassivemultitasklanguage}}
    \label{fig:MMLU-Sample-Questions}
\end{figure}

According to the MMLU-PRO paper, \enquote{The questions in MMLU are mostly knowledge-driven without requiring too much reasoning, especially in the STEM subjects, which reduces its difficulty. In fact, most models achieve better performance with \enquote{direct} answer prediction without chain-of-thought}

The following benchmarks are provided by the MMLU-PRO paper:

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\textwidth]{MMLU_PRO.png}
    \caption{MMLU-PRO benchmark results for open and closed source models \parencite{hendrycks2021measuringmassivemultitasklanguage}}
    \label{fig:MMLU-PRO}
\end{figure}

In these benchmarks, it is obvious that the top 7 closed-source models were measured to be more perfomant than the top 7 open source models based on overall percentage scores for number of questions answered correctly from the MMLU-PRO question bank. This is a significant finding, but I believe that it isn't as applicable to the competition between open and closed source models today. This paper was made on November 6th, 2024. Since then a multitude of newer models have come out.

\textbf{GPQA \texorpdfstring{(Google-Proof Q\&A Benchmark)}} is a benchmark focused on making multiple choice problems that are difficult to answer even by PHD researchers. These questions are all considered graduate-level and are intended for college students who are either pursuing a masters or PhD degree.\\
 Additionally, a subset of these problems, called a "diamond" set was created to only include questions where experts in the field answer correctly, and a majority of non-experts in the field answer incorrectly. This subset of problems also has a requirement that involves multiple experts either answering correctly, or one expert answering incorrectly while the other answers correctly, and the expert that answers incorrectly having the ability to explain their mistake after seeing the answer. This unique set of constraints makes the GPQA a suitable set for difficult, but fair graduate level questions.

According to the paper, one of the most robust models at the time (GPT-4 with search) scored a 38.8\% on the diamond set \parencite{rein2023gpqagraduatelevelgoogleproofqa}. To put this in perspective of how far AI models have come since November 2023, Gemini 3.0 Pro scored 91.8\% on the same set of questions (shown in Figure~\ref{fig:gemini-benchmarks}).

\subsection{Cybersecurity Related Benchmarks}

\subsubsection{CyberMetric}

CyberMetric is one of the early approaches to determine how effective humans are at solving multiple-choice cybersecurity related problems versus LLMs. The authors of the paper created a list of 10,000 questions that were generated by AI (GPT-3.5 turbo) by retrieving relevant questions from the internet using RAG, and generating a question and multiple answers to turn these cybersecurity concepts into multiple choice questions. There was a wide variety of types of questions that this multiple choice set included. The images below show the different cyber related topics this paper aimed to cover, and the research questions this paper aims to address:

\begin{figure}[H]
    \centering

    \begin{subfigure}[t]{0.48\textwidth}
        \centering
        \includegraphics[width=\linewidth]{CyberMetric_research_questions.png}
        \caption{CyberMetric research questions}
        \label{fig:CyberMetric-research-questions}
    \end{subfigure}\hfill
    \begin{subfigure}[t]{0.48\textwidth}
        \centering
        \includegraphics[width=\linewidth]{CyberMetric_Questions.png}
        \caption{CyberMetric cybersecurity topics}
        \label{fig:CyberMetric-topics}
    \end{subfigure}

    \caption{CyberMetric research methods \parencite{tihanyi2024cybermetricbenchmarkdatasetbased}}
    \label{fig:CyberMetric-questions-and-topics}
\end{figure}


The authors of this paper first conducted an experiment on a subset of cybersecurity questions taken from the original 10,000 that were generated by ChatGPT. They asked 30 participants to solve only 80 of these questions. This subset of 80 questions was manually verified by cybersecurity professionals to ensure they were relevant and accurate. The participants with more education in cybersecurity based on degree (PhD vs Ba/Ms degrees) performed better than people who didn't have as much education. This verified the validity of the dataset.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.2\textwidth]{CyberMetric_accuracy.png}
    \caption{CyberMetric benchmarks validating dataset. \parencite{tihanyi2024cybermetricbenchmarkdatasetbased}}
    \label{fig:CyberMetric-dataset}
\end{figure}

The LLMs then were tested against the multiple-choice questions in the CyberMetric benchmark by measuring the percentage of questions they answered correctly. They tested different subsets of the question bank (80 Qs, 500 Qs, 2k Qs, 10k Qs) to ensure that the LLMs perform similarly on the different subsets of questions. The CyberMetric-80 and CyberMetric-500 datasets were fully verified by human experts according to the paper.

The result of the study was that this test concluded that machine intelligence has already surpassed humans in answering questions related to cybersecurity. This was the finding of the answer to the first research question, and the top performing model was GPT-4o which is a closed-source, proprietary model. According to the paper, these were the results:

\begin{figure}[H]
    \centering

    \begin{subfigure}[t]{0.48\textwidth}
        \centering
        \includegraphics[width=\linewidth]{human_vs_llm_performance.png}
        \caption{Comparing Human vs LLM performance on CyberMetric-80}
        \label{fig:CyberMetric_human_vs_llm_performance}
    \end{subfigure}\hfill
    \begin{subfigure}[t]{0.48\textwidth}
        \centering
        \includegraphics[width=\linewidth]{llm_performance_CyberMetric.png}
        \caption{Ranking LLM performance on CyberMetric 80Qs, 500Qs, 2k Qs, 10k Qs}
        \label{fig:CyberMetric_llm_performance}
    \end{subfigure}

    \caption{CyberMetric study results \parencite{tihanyi2024cybermetricbenchmarkdatasetbased}}
    \label{fig:2_3}
\end{figure}

The experiment in this report provides a unique comparison to the results in the CyberMetric report because as of today, closed and open source models are not only better than humans at solving cybersecurity challenges, but they are both starting to score very high on multiple benchmarks. In this report, it is shown that the open source model "Mixtral" scores almost as good as GPT 4o in the image above. Nowadays, there are far more advanced open and closed source models, and I will determine if this result is consistent with more recent models and ctf challenges.

\subsubsection{CyberSecEval 2}

The CyberSecEval 2 benchmark was created to test 3 major aspects of LLM security: 

\paragraph{prompt injection evaluation}
The first aspect involved testing for prompt injection. For example, if the user gave an LLM a secret key, and then later asked for the LLM to repeat that secret key, the benchmark would measure whether the LLM would actually repeat that secret key or refuse to answer the user's question based on the privacy implications of leaking a secret key.

\paragraph{vulnerability exploitation evaluation}
The second type of test, which is what our focus will be on in this report, is the ability for LLMs to exploit vulnerabilities. In order to measure this, the creators of the experiment used ctf challenges that were not too challenging, but challenging enough so that LLMs could solve them at least some of the time. In the paper, it also focused on drawing inspiration from actual vulnerabilities in software.

\paragraph{code interpreter abuse evaluation}
The last type of vulnerability that this benchmark evaluated is the willingness of an LLM to execute vulnerable code inside of a code interpreter. This can be crucial for things like an LLM executing code that it downloads from an untrusted website. It can be easy to trick a human into executing malicious programs or scripts downloaded from the internet, and with careful prompt engineering, it is most likely even easier to trick an LLM into doing this. In order to test this, the authors of the paper created a set of prompts that try to manipulate the LLM into running malicious code to take control of the system that the LLM is running on.

The results from the vulnerability and exploitation evaluation in this study are shown below:

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{exploit_security_concepts_cysec_eval_2.png}
    \caption{Exploitation capability scores broken down by model and test category. \parencite{bhatt2024cyberseceval2widerangingcybersecurity}}
    \label{fig:cysec-eval-2}
\end{figure}

According to the results of this experiment, it seems that LLMs struggled solving some CTF challenges like SQL injection ctf challenges, and also basic buffer overflow challenges. Additionally, the closed-source GPT-4-turbo model performed the best in solving these challenges. Since this paper was created in April, 2024, I believe the results coming out of this experiment may differ from the results in this report. I am going to be testing newer, more robust open source models and one of the challenges I will be testing on will involve a sql injection vulnerability.

\subsubsection{NYU Cyber Bench}

NYU CTF Bench is a specialized benchmark dataset designed specifically for evaluating large language models in offensive security tasks. Unlike general cybersecurity benchmarks that focus on theoretical knowledge or simplified vulnerability detection, NYU CTF Bench provides a comprehensive evaluation framework that more closely mirrors real-world cybersecurity scenarios. It is one of the first CTF benchmarks to include a large sample size of challenges in order to accurately measure LLM performance against hard ctf challenges.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\textwidth]{NYU_CTF_Bench_Vs_Others.png}
    \caption{Comparison of LLM-Driven CTF Solving. \parencite{shao2025nyuctfbenchscalable}}
    \label{fig:NYU-Cyber-Bench_Models}
\end{figure}


What distinguishes NYU CTF Bench from other benchmarks is its focus on interactive cybersecurity challenges. The benchmark utilizes Capture The Flag (CTF) challenges sourced from the CSAW competitions, which are well-established in the cybersecurity community for testing practical offensive security skills \parencite{shao2025nyuctfbenchscalable}. \\
These challenges require multi-step reasoning, vulnerability identification, and exploit development. \\
The benchmark allows researchers to assess not only whether LLMs can solve security challenges, but also how effectively they can plan and execute multi-step attack sequences.



The benchmark also uses open-source models in their experimentation, but the problem is that it is comparing old, dated open source models to robust proprietary models. Since the time that this article was written, open-source models have advanced quite a bit. In the article, it uses Mixtral and Llama as representative open-source models, and neither of them solve any challenges according to the results below. Before this paper was releaased in February 2025, Deepseek was released in January 2025. Deepseek was one of the first robust open-source models, and its released even caused Nvidia stock to fall due to how good it was compared to top proprietary models. In this report, I will be using this Deepseek model, along with others that have came out since then that are even more robust.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\textwidth]{NYU_CTF_Bench_Table.png}
    \caption{Performance and Failure Rates of Different LLMs. \parencite{shao2025nyuctfbenchscalable}}
    \label{fig:NYU-Cyber-Bench-Performance}
\end{figure}


\subsubsection{Cybench}

Cybench is a comprehensive benchmark developed by Stanford University for evaluating the cybersecurity capabilities of language models through practical Capture The Flag (CTF) challenges \parencite{zhang2025cybench}. What makes Cybench unique is its use of 40 professional-level CTF tasks sourced from four distinct competitions: HackTheBox 2024, SekaiCTF 2022-23, Glacier 2023, and HKCert 2023 \parencite{zhang2025cybench}.

Unlike other cybersecurity benchmarks that rely on theoretical knowledge questions or simplified vulnerability detection, Cybench evaluates language model agents in realistic environments where they must autonomously identify vulnerabilities, develop exploits, and execute them to capture flags \parencite{zhang2025cybench}. Each task includes a complete environment setup with task descriptions, starter files, and evaluators that verify successful flag submissions.

\begin{figure}[H]
\centering
\includegraphics[width=0.9\textwidth]{cybench_agents.png}
\caption{Overview of the agent flow. An agent acts on memory $m_t$, consisting of the initial prompt
$m_0$ and the last three responses and observations $r_{t\text{-}3}, o_{t\text{-}3}, r_{t\text{-}2}, o_{t\text{-}2}, r_{t\text{-}1}, o_{t\text{-}1}$ to produce a
response $r_t$ and an action $a_t$. It then executes action $a_t$ on environment $s_{t\text{-}1}$ to yield an observation
$o_t$ and updated environment $s_t$. It finally updates its memory for the next timestamp using response
$r_t$ and observation $o_t$ to produce $m_{t+1}$.
\parencite{zhang2025cybench}}
    \label{fig:cybench-agents}
\end{figure}

The key innovation of Cybench is its introduction of subtasks that break complex challenges into intermediate steps, enabling more granular assessment of agent capabilities and partial credit evaluation \parencite{zhang2025cybench}. This approach allows researchers to identify exactly where language models fail in multi-step exploitation processes.

The results of the experiment these researchers at Stanford did is shown below:

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{cybench_results.png}
    \caption{Performance and Failure Rates of Different LLMs. \parencite{zhang2025cybench}}
    \label{fig:Cybench-Performance}
\end{figure}

The Cybench evaluation revealed that even the best-performing models (Claude 3.5 Sonnet at 17.5\% and GPT-4o at 12.5\%) could only solve tasks with first solve times of 11 minutes or less, with none successfully solving challenges that took human teams longer than 11 minutes \parencite{zhang2025cybench}. This demonstrates the significant gap between current language model capabilities and human expertise in complex cybersecurity challenges. Additionally, it showed that LLMs even struggled with "subtask-guided performance" which involves breaking a problem down into pieces before attempting to solve it, then giving those separate steps and pieces to the LLM to solve by itself, and piece together those steps to solve the entire challenge. The LLMs were able to solve the subtasks pretty effectively compared to piecing together the subtasks to solve the challenge.

Cybench has been adopted by major AI safety organizations including the US and UK AISI, Anthropic, Amazon, and OWASP, establishing it as the current state-of-the-art benchmark for evaluating language model cybersecurity capabilities \parencite{zhang2025cybench}.

Even though this paper provides an in-depth view into how LLMs work with solving multi-step CTFs similar to what this report will provide, this benchmark was created in April 2025, and it also doesn't include some of the most robust open-source models that will be featured in this report like Kimi, GLM, and Deepseek.
\subsection{Gemini 3.0}

In November 2025, Gemini 3.0 was released and it beat almost all benchmarks. The only major benchmark it didn't outright beat was the SWE-bench verified benchmark.

These are the results of the Gemini 3.0 benchmarks:

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\textwidth]{gemini_benchmarks.png}
    \caption{Gemini 3.0 benchmark results \parencite{google_gemini3_2025}}
    \label{fig:gemini-benchmarks}
\end{figure}


Some users of the model don't believe it is as good as it claims for some of these benchmarks. There is a term in AI research called benchmarkmaxing where companies only focus on doing well on these benchmarks, and not on having performant models. Some users think this could be the case for Gemini 3.0.

\subsection{How AI Models Think}

Large Language Models do not “think” in a human sense, but they do perform multi-step inference processes that can resemble reasoning. Modern frontier models increasingly rely on structured chains of intermediate steps, reward-optimized reasoning loops, and internal “self-prompting” mechanisms that guide multi-step inference. These processes determine not only how a model solves complex tasks like CTF challenges, but also how reliably it can explain and justify its answers.

Two major paradigms have emerged in the design of reasoning-capable LLMs: implicit thinking and explicit thinking. Understanding the distinction between the two is critical for evaluating correctness, security, reproducibility, and susceptibility to reasoning errors such as hallucination or overthinking.

Chain-of-Thought (CoT) is a prompting technique in which a model is instructed to generate step-by-step intermediate reasoning before giving its final answer. Both implicit and explicit thinking use CoT to derive the final answer, but In practice, explicit-thinking models tend to produce CoT-like traces automatically, while implicit-thinking models often perform similar multi-step reasoning internally but suppress these steps in the final output. Thus, CoT serves as an observable proxy for explicit reasoning: when CoT is visible, the reasoning is transparent and auditable; when it is hidden, the model’s reasoning occurs implicitly, making validation more difficult.

This is an example of Chain-of-Thought prompting from the official paper,

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\textwidth]{chain_of_thought.png}
    \caption{Chain of Thought Thinking Process}
    \label{fig:cot-thinking}
\end{figure}

In the image above, the first box of each prompting method is the prompt that was used for the AI. The box below it is response from the AI model. the chain-of-thought prompt comes up with the correct answer to the user question, while the standard prompting gives an incorrect answer. This is because with chain-of-thought, the model is being given context about how to reason about solving a problem. It then can apply this reasoning method to actually solving the problem, instead of trying to solve it directly. This is how humans mostly solve problems, they reason through it before directly giving an answer. Modern AI models are adopting this method of "reasoning" more and more which is what makes them so powerful. They adopt CoT thinking by using either implicit or explicit thinking methods. \parencite{cot_thinking_2022}

\subsubsection{Implicit vs Explicit Thinking}

In the following article, the tradeoff in explicit vs implicit thinking model,

According to DigAI Lab,

"Implicit reasoning inherently suppresses intermediate outputs, rendering the underlying computation process opaque. This lack of visibility prevents us from knowing whether the model is performing genuine multi-step reasoning or merely exploiting memorized knowledge and spurious correlations."\parencite{li2025implicitreasoninglargelanguage}

Overall, Explicit reasoning generally provides more fine-grained logical structure, which can reduce hallucination in multi-step tasks by forcing the model to commit to intermediate steps. Implicit reasoning, by contrast, relies on latent internal states, which can make errors harder to detect and can lead to confident but incorrect conclusions. In the methods of this paper, both explicit and implicit thinking models will be tested. Explicit-thinking models may be particularly valuable for cybersecurity organizations because they provide detailed, step-by-step reasoning traces. These logs allow analysts to audit how the model arrived at its conclusions, verify that each step is logically sound, and detect any hallucinations or incorrect assumptions that may influence the final answer.

In this paper, we will be testing a model that utilizes lots of explicit thinking (Kimi K2 Thinking) against a model that uses implicit thinking (Kimi K2 0905). Comparing these models isn't exactly apples-to-apples, and the performance of Kimi K2 Thinking may outperform Kimi K2 0905 purely because it is newer and more robust in general, but this will give a glimpse into what the explicit thinking model comes up with compared to a very similar implicit thinking model.

The different thinking mechanisms along with the specific AI models used in this paper, are shown in table~\ref{tab:models}. It is seen that most of the good proprietary models, and all of the proprietary models used in this report are implicit thinking models. This is partially because companies don't want to leak the reasoning processes of their models to other companies to train off of.

\section{Methods}

\subsection{Using External Providers}

For my experimentation on whether open-source LLMs can effectively replace closed-source LLMs for solving company cybersecurity problems, I will be using external providers that give me access to models hosted on their platforms. All of the open source models I will be showcasing do have the capabilities of being ran locally.

\subsubsection{Openrouter}

Openrouter is a platform that provides API access to almost every publicly available large language model. There are companies and services that provide compute to Openrouter, along with access to an LLM model. In exchange, these services get paid for every token that Openrouter generates.

\begin{figure}[H]
\centering
\includegraphics[width=0.5\textwidth]{openrouter.png}
\caption{Openrouter API to use External Providers Instead of Hosting Open Source LLMS Locally}
    \label{fig:openrouter_platform}
\end{figure}

Openrouter has access to both open source and proprietary models, but when using a proprietary model, the only provider that will be available is the one coming from the official company providing the services for that model. For open source models, the different providers and pricing will vary, because these models are free to the public to host. 

\begin{figure}[H]
    \centering

    \begin{subfigure}[t]{0.48\textwidth}
        \centering
        \includegraphics[width=\linewidth]{GPT_Providers.png}
        \caption{OpenAI Providing the ChatGPT model}
        \label{fig:OpenAI-GPT-5.1}
    \end{subfigure}\hfill
    \begin{subfigure}[t]{0.48\textwidth}
        \centering
        \includegraphics[width=\linewidth]{Deepseek_Providers.png}
        \caption{Deepseek Providers for Deepseek R1 0528}
        \label{fig:Deepseek-Providers}
    \end{subfigure}

    \caption{Different models available and Openrouter and their providers}
    \label{fig:Openrouter-Providers}
\end{figure}

In the images above, the only available provider for OpenAI's GPT 5.1 is OpenAI, the providers for Deepseek R1 include Parasail, DeepInfra, SiliconFlow, and NovitaAI, and many more. This shows the advantage to using open source. The end user can either host their own models, or use cheap external providers to get solid value for their cost.

\subsubsection{Continue.dev}

Continue.dev is an open source VSCode extension and also offers a CLI to allow AI to interact with your filesystem. I will be using this tool to allow LLMs to read my CTF files, and have a consistent prompt to test all LLMs equally.

Continue.dev supports both open and closed source LLMs, and supports any OpenAI API compatible model to connect to it.

\begin{figure}[H]
    \centering

    \begin{subfigure}[t]{0.48\textwidth}
        \centering
        \includegraphics[width=\linewidth]{vscode_extension.png}
        \caption{Continue.dev VSCode Extension}
        \label{fig:4}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.48\textwidth}
        \centering
        \includegraphics[width=\linewidth]{models.png}
        \caption{Continue.dev Supported Models}
        \label{fig:5}
    \end{subfigure}

    \caption{Continue.dev features}
        \label{fig:4_5}
    \end{figure}

    \subsection{Challenge Files and Standardized Prompt for All LLMs}
    In table~\ref{tab:models}, the different LLMs that will be used in these experiments are displayed. These LLMs will be provided a standard script to solve each individual challenge:

    \subsubsection{SmileyCTF 2025 -- SaaS (Cryptography)}
    For the cryptography challenge Saas from SmileyCTF 2025, the following prompt will be used:

    "The code files you have been given contain a cryptography CTF challenge. Can you explain this challenge, and write me a file called solve.py that will solve it? Use pwntools in your solver script to interact with the remote server to get the flag. Assume that the remote server is on 127.0.0.1. You will only be given these files, and nothing more to solve the challenge. Do not assume any additional values or hints will come from the challenge server. Everything you need to solve the challenge is in the files."
\begin{figure}[H]
\dirtree{%
  .1 saas/.
  .2 chall.py.
  .2 docker-compose.yml.
  .2 Dockerfile.
  .2 flag.txt.
}
\caption{SaaS challenge file structure provided to LLMs}
\label{fig:crypto-saas-structure}
\end{figure}
    
    

    \begin{table}[H]
\centering
\caption{Classification of LLMs by Reasoning Transparency}
\label{tab:models}
\begin{tabular}{lcc}
\toprule
\textbf{Model} & \textbf{Thinking Type} & \textbf{Release Date} \\
\midrule
GLM-4.6                & Explicit & 2025-09 \\
Kimi K2 Thinking       & Explicit & 2025-11 \\
DeepSeek R1            & Explicit & 2025-01 \\
Kimi K2 0905           & Implicit & 2025-09 \\
Deepseek V3.2 Speciale & Explicit & 2025-12 \\
\midrule
GPT-5.1                & Implicit & 2025-11 \\
Claude Sonnet 4.5      & Implicit & 2025-02 \\
Grok 4.1 Fast          & Implicit & 2025-11 \\
Gemini 3.0 Pro Preview & Implicit & 2025-11 \\
GPT-5.1 Codex          & Implicit & 2025-11 \\
\bottomrule
\end{tabular}
\end{table}
  
  I will use the following prompt to all Artificial Intelligence models in the 
  
    \section{Results}
    
    \subsection{SmileyCTF 2025 -- SaaS (Cryptography)\quad\textit{(Difficulty: Easy)}}

    \subsubsection{Challenge Code}
    
    Consider the following cryptography challenge from SmileyCTF 2025 which is a difficult challenge for beginners in the cryptography space to solve, but somewhat simple for experts. During the CTF, this challenge was marked as "easy" difficulty, but SmileyCTF is generally known to have very difficult challenges.
\begin{figure}[H]
\centering

\end{figure}
\newpage

    
    \subsubsection*{Solution Approach}
    
    See appendix A for the complete solution, along with a high level overview of the approach needed to solve the problem. This challenge was provided by SmileyCTF \parencite{ctf-gg_smileyctf-2025_2025}. The official challenge code can be found on \href{https://github.com/ctf-gg/smileyctf-2025/blob/main/crypto/saas/chall/chall.py}{SmileyCTF's github}.

    \subsubsection*{LLM Results}

    Open source and proprietary models will be tested in this section. The first 4 models are the different open-source models to test.

    \subsection{Hackthebox - MSS (Cryptography)\quad\textit{(Difficulty: Easy)}}

    \subsubsection{Challenge Code}

    This challenge was provided by Hackthebox \parencite{uni_ctf_2023}. See appendix B, for a high level approach on the general solution to solve the problem. The solution can be found on \href{https://github.com/hackthebox/uni-ctf-2023/blob/main/uni-ctf-2023/crypto/%5BEasy%5D%20MSS/htb/solver.py}{HackTheBox's github}. The challenge code is only available to VIP subscribers on the platform.
    
    \subsubsection*{LLM Results}

    \section{Discussion: Model Context Protocol in Cybersecurity and CTFs}
    
    The Model Context Protocol (MCP) is a standardized interface that allows Large Language Models (LLMs) to securely call functions from external systems. Introduced as an open-source standard in 2024, MCP serves as a universal translator between AI models and security tools, enabling seamless integration and automation of cybersecurity workflows. This protocol transforms how AI systems can access, control, and analyze cybersecurity infrastructure, opening new possibilities for both offensive and defensive security operations.
    
    HackTheBox has developed an open-source MCP implementation that provides LLMs with access to CTF challenges and automated testing scripts within a structured environment. Their implementation offers a standardized interface for connecting AI assistants to the HackTheBox platform functionality, enabling seamless integration of AI into CTF competitions and cybersecurity training \parencite{hackthebox_mcp_server}. This open-source solution demonstrates how MCP can be practically applied to cybersecurity challenges by creating a controlled environment where LLMs can interact with pre-configured cybersecurity challenges, rather than providing direct access to external security tools like Nmap or Burp Suite.
    
    According to HackTheBox, "MCP provides a standardized way to plug modern capabilities into competitions, making CTFs a closer mirror of real-world security operations" \parencite{hackthebox_mcp_server}. This standardization is particularly valuable when comparing open-source and closed-source models, as it ensures that any performance differences are due to the models' inherent capabilities rather than environmental factors or implementation advantages.

    \subsection{Standardized Testing Environments for Model Evaluation}
    
    The integration of MCP servers with CTF platforms creates a standardized testing environment that provides more accurate evaluation for both open-source and closed-source models. Unlike traditional benchmarks that may favor one model architecture over another, MCP-based CTF environments establish a level playing field where all models interact with identical challenges through the same standardized interface. This approach addresses several key limitations in current LLM evaluation methodologies:
    
    \begin{itemize}
      \item \textbf{Consistent Environment Access}: All models access the same challenges through identical MCP interfaces, eliminating variations in tool access or implementation differences that could skew results
      \item \textbf{Reproducible Testing Conditions}: MCP servers ensure that each model faces the same challenge parameters, time constraints, and resource limitations, enabling fair comparison between open and closed-source systems
      \item \textbf{Real-World Complexity}: Unlike simplified benchmarks, CTF challenges accessed through MCP maintain the complexity and multi-step nature of actual cybersecurity scenarios, providing more meaningful performance metrics
      \item \textbf{Transparent Evaluation}: The standardized nature of MCP interfaces allows researchers to observe exactly how each model approaches problems, making it easier to identify strengths and weaknesses across model types
    \end{itemize}
        
    The emerging ecosystem of MCP servers for cybersecurity evaluation includes several specialized implementations that further enhance standardized testing:
    
    \begin{itemize}
      \item \textbf{Kali MCP Server}: Acts as a bridge to Kali Linux environments, allowing AI models to access professional security tools through a standardized interface, ensuring consistent tool access regardless of whether the model is open-source or closed-source \parencite{wh0am1232024kalimcpserver}
      \item \textbf{DeepEval MCP Framework}: Provides standardized evaluation primitives for testing LLM performance across different cybersecurity tasks, enabling fair comparison between model architectures \parencite{confident2024deepeval}
    \end{itemize}
    
    These implementations demonstrate how MCP servers create a unified testing methodology that applies equally to both open-source and closed-source models. By providing identical interfaces to the same security tools and challenges, MCP environments eliminate the advantages that might arise from proprietary integrations or specialized optimizations that favor one model type over another.

    \subsection{Real-World Implementations}
    
    HackTheBox's MCP server exemplifies how this technology is transforming cybersecurity education and practice. Their implementation provides AI assistants with programmatic access to the HackTheBox platform's CTF challenges and automated testing environments, enabling seamless integration of AI into CTF competitions and cybersecurity training \parencite{hackthebox_mcp_server}.
    
    According to Hackthebox:
    
    "We’re laying the foundation for a new mode of competition where speed, automation, and machine intelligence become part of the game. At Hack The Box we always champion “learning by doing,” and now “doing” includes leveraging advanced tools and AI to enhance your capabilities, automate tedious processes, and reinvent how challenges are approached" \parencite{hackthebox_mcp_server}.
    
    Key capabilities include:
    
    \begin{itemize}
      \item \textbf{Seamless Event Management}: Access and explore all available CTF events via the MCP interface, with real-time visibility into participants and active challenges
      \item \textbf{Performance Insights}: Monitor team scores, retrieve challenge solve data, and analyze success rates to guide strategy
      \item \textbf{Challenge Lifecycle Automation}: Start, stop, and monitor challenge environments automatically, reducing manual operations during high-stakes competitions
      \item \textbf{Flag Submission}: Submit flags through validated, automated processes with immediate feedback and scoring confirmation
    \end{itemize}
    
    The impact of this integration is already evident in the cybersecurity community, with research showing that nearly two-thirds (63\%) of CTF participants are already using AI tools like ChatGPT or GitHub Copilot during events \parencite{hackthebox_mcp_server}. MCP support lowers entry barriers for newcomers while enabling experienced practitioners to automate tedious processes and focus on strategic thinking.
    
    \subsection{Structured Interfaces for Security Tools}
    
    MCPs provide standardized interfaces for a wide range of cybersecurity tools, transforming how AI systems interact with security infrastructure. The "Awesome Cyber Security MCP" repository catalogs numerous implementations that demonstrate the protocol's versatility across the security domain \parencite{awesome_cyber_security_mcp}:
    
    \begin{itemize}
      \item \textbf{Web Application Security}: Burp Suite MCP enables AI-driven web application testing by connecting Burp Suite to AI clients through MCP, allowing for automated vulnerability discovery and exploitation \parencite{burp_suite_mcp}
      \item \textbf{Network Reconnaissance}: Tools like Nuclei MCP provide fast vulnerability scanning capabilities, while Shodan MCP offers AI access to comprehensive internet-connected device search and CVE information
      \item \textbf{Reverse Engineering}: Ghidra MCP and IDA Pro MCP enable autonomous binary analysis, allowing AI systems to perform complex reverse engineering tasks with minimal human guidance
      \item \textbf{Password Recovery}: Hashcat MCP provides natural language-driven hash cracking, making password recovery more accessible to security analysts
    \end{itemize}
    
    These implementations demonstrate how MCPs create a unified ecosystem where AI can leverage specialized security tools without requiring deep knowledge of each tool's specific interface or implementation details. This helps both open-source and proprietary models perform better in security ctf challenges, and in the real world.

    \subsection{Future Benchmarks}

    The performance of LLMs on CTFs must be further studied by giving LLMs access to this an MCP protocol that allows them to analyze the challenge, and test different solutions autonomously. The future of benchmarking these LLMs in order to determine how well they are able to solve CTF challenges should involve MCP access to a wide variety of tools and services. These tools allow the LLM to solve a problem similarly to how a human would, and therefore they can better demonstrate how they can solve real-world challenges instead of giving the LLM the code of the challenge in a prompt with no added context, and asking it to solve it. When an LLM doesn't know how to solve a problem, it can try different approaches by using a variety of tools like Ghidra for vulnerable binaries, and Burp Suite for testing vulnerable web applications. 
\newpage


\printbibliography

\newpage

\appendix
\section{Mathematical Derivation for SmileyCTF 2025 SaaS Challenge}

\subsection*{Square roots modulo \(n\) and factor recovery}

Let \(p\) and \(q\) be distinct primes with \(p \equiv q \equiv 3 \pmod{4}\), and let \(n = pq\).
For an input \(x \in \mathbb{Z}_n\), the oracle computes square roots modulo each prime and recombines them via CRT.

\subsection*{Square roots modulo a prime}

Let \(p\) be an odd prime with \(p \equiv 3 \pmod{4}\).
If \(x\) is a quadratic residue modulo \(p\) (and \(x \not\equiv 0 \pmod{p}\)), Euler's criterion gives
\[
x^{\frac{p-1}{2}} \equiv 1 \pmod{p}.
\]
Consider
\[
\left(x^{\frac{p+1}{4}}\right)^2
= x^{\frac{p+1}{2}}
= x \cdot x^{\frac{p-1}{2}}
\equiv x \cdot 1
\equiv x \pmod{p},
\]
so \(x^{\frac{p+1}{4}}\) is a square root of \(x\) modulo \(p\):
\[
\boxed{\left(x^{\frac{p+1}{4}}\right)^2 \equiv x \pmod{p}}.
\]

If \(x\) is not a quadratic residue, then
\[
\left(x^{\frac{p+1}{4}}\right)^2 \equiv -x \pmod{p}.
\]

Thus, in all non-degenerate cases,
\[
r_p^2 \equiv \pm x \pmod{p},
\qquad
r_q^2 \equiv \pm x \pmod{q}.
\]

\subsection*{CRT recombination of square roots modulo \(n\)}

Let
\[
r_p \equiv x^{\frac{p+1}{4}} \pmod{p},
\qquad
r_q \equiv x^{\frac{q+1}{4}} \pmod{q},
\]
and define
\[
A \equiv q(q^{-1} \bmod p),
\qquad
B \equiv p(p^{-1} \bmod q).
\]
Then for independent signs \(a,b\in\{\pm1\}\),
\[
r_{a,b} \equiv a r_p A + b r_q B \pmod{n}.
\]

\subsection*{Recovering \(n\) from complementary roots}

If two roots correspond to opposite signs, say \((a,b)\) and \((-a,-b)\), then
\[
r_{a,b} + r_{-a,-b} \equiv 0 \pmod{n}.
\]
Taking representatives in \([0,n-1]\),
\[
r_{-a,-b} = n - r_{a,b}.
\]
Thus, after sorting the four roots,
\[
\boxed{n = r_{\min} + r_{\max}}.
\]

\subsection*{Extracting a prime via a GCD}

Consider a pair differing only in the \(q\)-component:
\[
r_{a,b} = a r_p A + b r_q B,
\qquad
r_{a,-b} = a r_p A - b r_q B.
\]
Their difference is
\[
r_{a,b} - r_{a,-b} = 2b\, r_q B.
\]
Reducing modulo the primes:
\[
r_{a,b}-r_{a,-b} \equiv 0 \pmod{p},
\qquad
r_{a,b}-r_{a,-b} \equiv 2b r_q \pmod{q}.
\]

Thus,
\[
\boxed{\gcd(r_{a,b}-r_{a,-b},\, n) = p}.
\]
Similarly, a pair differing only in the \(p\)-component yields \(q\).
In practice, collect the four roots and compute:
\[
p = \gcd(r_i - r_j,\, n), \qquad q = \frac{n}{p}.
\]

\subsection*{Forge the signature}

Once \(p\) and \(q\) are known:
\[
\varphi(n) = (p-1)(q-1),
\qquad
d \equiv e^{-1} \pmod{\varphi(n)}.
\]
Given the challenge value \(m\), the RSA signature is:
\[
\boxed{s \equiv m^d \pmod{n}}.
\]

Submitting \(s\) satisfies \(s^e \equiv m \pmod{n}\) and reveals the flag.

\subsection*{Solution Code}

\lstinputlisting[
    style=mypython,
    caption={Example solve script},
]{CodeFiles/solvers/saas/solve.py}

\section{Mathematical Derivation for HackTheBox MSS Challenge Solution}

The server provides two main operations:
\begin{enumerate}
  \item \texttt{get\_share(x)}: Returns a share of the secret as a point on a polynomial of degree 30
  \item \texttt{encrypt\_flag()}: Returns the AES-encrypted flag using SHA256 of the key
\end{enumerate}

The polynomial is defined as:
\[f(x) = a_0 + a_1x + a_2x^2 + \ldots + a_{30}x^{30}\]
where \(a_0 = \text{key}\).

The challenge limits us to only 19 shares, but the polynomial has degree 30, making direct polynomial interpolation impossible.

\subsection*{Mathematical Foundation}

The key insight is that if we evaluate the polynomial modulo \(x\), we get:
\[f(x) \equiv a_0 \pmod{x}\]

This means for any prime \(p\):
\[\text{key} \equiv f(p) \pmod{p}\]

By collecting multiple such congruences with different primes, we can use the Chinese Remainder Theorem (CRT) to recover the original key.

\subsection*{Chinese Remainder Theorem Application}

Given a system of congruences:
\[\begin{cases}
x \equiv a_1 \pmod{m_1} \\
x \equiv a_2 \pmod{m_2} \\
\vdots \\
x \equiv a_n \pmod{m_n}
\end{cases}\]

where \(m_1, m_2, \ldots, m_n\) are pairwise coprime, CRT guarantees a unique solution modulo \(M = m_1m_2\ldots m_n\).

For this challenge, we:
\begin{enumerate}
  \item Select 19 different 15-bit primes
  \item For each prime \(p_i\), compute \(a_i = f(p_i) \bmod p_i\)
  \item Apply CRT to find the unique solution \(x\) such that \(x \equiv a_i \pmod{p_i}\) for all \(i\)
  \item Ensure the product of all primes exceeds the key size p (256 bits)
\end{enumerate}

\subsection*{Implementation Approach}

\subsection*{Solution Code}

View \href{https://github.com/hackthebox/uni-ctf-2023/blob/main/uni-ctf-2023/crypto/%5BEasy%5D%20MSS/htb/solver.py}{HackTheBox's github} for the official solution to the problem.

\end{document}



